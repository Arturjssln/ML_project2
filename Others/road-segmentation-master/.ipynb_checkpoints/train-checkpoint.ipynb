{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from cross_validation import *\n",
    "\n",
    "# Load the training set\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-efabbdbfe544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from naive_model import NaiveModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCnnModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#from logistic_model import LogisticModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#model = NaiveModel()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\road-segmentation-master\\cnn_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#from naive_model import NaiveModel\n",
    "from cnn_model import CnnModel\n",
    "#from logistic_model import LogisticModel\n",
    "\n",
    "#model = NaiveModel()\n",
    "model = CnnModel()\n",
    "#model = LogisticModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 72, 72)    4864        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_1 (LeakyReLU)          (None, 64, 72, 72)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 36, 36)    0           leakyrelu_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64, 36, 36)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 36, 36)   73856       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_2 (LeakyReLU)          (None, 128, 36, 36)   0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 18, 18)   0           leakyrelu_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128, 18, 18)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 256, 18, 18)   295168      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_3 (LeakyReLU)          (None, 256, 18, 18)   0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 9, 9)     0           leakyrelu_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256, 9, 9)     0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 256, 9, 9)     590080      dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_4 (LeakyReLU)          (None, 256, 9, 9)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 256, 5, 5)     0           leakyrelu_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256, 5, 5)     0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6400)          0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           819328      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_5 (LeakyReLU)          (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 128)           0           leakyrelu_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             258         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1783554\n",
      "____________________________________________________________________________________________________\n",
      "Training set shape:  (100, 400, 400, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpq6o68yjz/m9a6bd0eb5ed5c92e91261282fc495cb4.lib and object C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpq6o68yjz/m9a6bd0eb5ed5c92e91261282fc495cb4.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmp99pr800a/mcaee517fdbbfe5601d70389b5e9a720a.lib and object C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmp99pr800a/mcaee517fdbbfe5601d70389b5e9a720a.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpj7mtstbw/m848dd898e26d545ff6290e3aa98de3d5.lib and object C:/Users/Dario/AppData/Local/Theano/compiledir_Windows-8.1-6.3.9600-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpj7mtstbw/m848dd898e26d545ff6290e3aa98de3d5.exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.4806 - acc: 0.7684   \n",
      "Epoch 2/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.3088 - acc: 0.8646   \n",
      "Epoch 3/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.2551 - acc: 0.8897   \n",
      "Epoch 4/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.2340 - acc: 0.9009   \n",
      "Epoch 5/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.2216 - acc: 0.9067   \n",
      "Epoch 6/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.2146 - acc: 0.9102   \n",
      "Epoch 7/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.2042 - acc: 0.9151   \n",
      "Epoch 8/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1950 - acc: 0.9196   \n",
      "Epoch 9/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1980 - acc: 0.9174   \n",
      "Epoch 10/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1907 - acc: 0.9230   \n",
      "Epoch 11/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1885 - acc: 0.9248   \n",
      "Epoch 12/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1844 - acc: 0.9256   \n",
      "Epoch 13/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1825 - acc: 0.9255   \n",
      "Epoch 14/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1776 - acc: 0.9285   \n",
      "Epoch 15/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1779 - acc: 0.9275   \n",
      "Epoch 16/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1760 - acc: 0.9292   \n",
      "Epoch 17/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1733 - acc: 0.9310   \n",
      "Epoch 18/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1734 - acc: 0.9300   \n",
      "Epoch 19/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1746 - acc: 0.9307   \n",
      "Epoch 20/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1697 - acc: 0.9314   \n",
      "Epoch 21/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1652 - acc: 0.9336   \n",
      "Epoch 22/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1712 - acc: 0.9324   \n",
      "Epoch 23/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1648 - acc: 0.9354   \n",
      "Epoch 24/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1651 - acc: 0.9341   \n",
      "Epoch 25/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1663 - acc: 0.9350   \n",
      "Epoch 26/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1675 - acc: 0.9348   \n",
      "Epoch 27/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1571 - acc: 0.9378   \n",
      "Epoch 28/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1552 - acc: 0.9386   \n",
      "Epoch 29/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1633 - acc: 0.9355   \n",
      "Epoch 30/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1607 - acc: 0.9366   \n",
      "Epoch 31/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1616 - acc: 0.9367   \n",
      "Epoch 32/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1580 - acc: 0.9380   \n",
      "Epoch 33/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1612 - acc: 0.9372   \n",
      "Epoch 34/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1567 - acc: 0.9390   \n",
      "Epoch 35/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1559 - acc: 0.9391   \n",
      "Epoch 36/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1571 - acc: 0.9384   \n",
      "Epoch 37/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1536 - acc: 0.9397   \n",
      "Epoch 38/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1573 - acc: 0.9399   \n",
      "Epoch 39/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1546 - acc: 0.9402   \n",
      "Epoch 40/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1540 - acc: 0.9403   \n",
      "Epoch 41/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1559 - acc: 0.9375   \n",
      "Epoch 42/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1531 - acc: 0.9408   \n",
      "Epoch 43/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1526 - acc: 0.9413   \n",
      "Epoch 44/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1535 - acc: 0.9396   \n",
      "Epoch 45/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1504 - acc: 0.9414   \n",
      "Epoch 46/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1497 - acc: 0.9430   \n",
      "Epoch 47/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1534 - acc: 0.9400   \n",
      "Epoch 48/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1461 - acc: 0.9438   \n",
      "Epoch 49/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1523 - acc: 0.9399   \n",
      "Epoch 50/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1532 - acc: 0.9411   \n",
      "Epoch 51/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1499 - acc: 0.9417   \n",
      "Epoch 52/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1480 - acc: 0.9440   \n",
      "Epoch 53/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1482 - acc: 0.9426   \n",
      "Epoch 54/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1477 - acc: 0.9423   \n",
      "Epoch 55/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1480 - acc: 0.9433   \n",
      "Epoch 56/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1475 - acc: 0.9425   \n",
      "Epoch 57/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1442 - acc: 0.9439   \n",
      "Epoch 58/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9423\n",
      "Epoch 00057: reducing learning rate to 0.0005000000237487257.\n",
      "62500/62500 [==============================] - 123s - loss: 0.1502 - acc: 0.9423   \n",
      "Epoch 59/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1323 - acc: 0.9481   \n",
      "Epoch 60/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1256 - acc: 0.9520   \n",
      "Epoch 61/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1269 - acc: 0.9513   \n",
      "Epoch 62/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1245 - acc: 0.9525   \n",
      "Epoch 63/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1266 - acc: 0.9519   \n",
      "Epoch 64/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1235 - acc: 0.9531   \n",
      "Epoch 65/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1224 - acc: 0.9535   \n",
      "Epoch 66/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1231 - acc: 0.9531   \n",
      "Epoch 67/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1237 - acc: 0.9533   \n",
      "Epoch 68/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1234 - acc: 0.9523   \n",
      "Epoch 69/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1255 - acc: 0.9532   \n",
      "Epoch 70/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1213 - acc: 0.9541   \n",
      "Epoch 71/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1219 - acc: 0.9537   \n",
      "Epoch 72/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1179 - acc: 0.9557   \n",
      "Epoch 73/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1191 - acc: 0.9545   \n",
      "Epoch 74/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1175 - acc: 0.9561   \n",
      "Epoch 75/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1155 - acc: 0.9556   \n",
      "Epoch 76/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1155 - acc: 0.9551   \n",
      "Epoch 77/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1184 - acc: 0.9547   \n",
      "Epoch 78/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1174 - acc: 0.9556   \n",
      "Epoch 79/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1168 - acc: 0.9554   \n",
      "Epoch 80/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9558\n",
      "Epoch 00079: reducing learning rate to 0.0002500000118743628.\n",
      "62500/62500 [==============================] - 123s - loss: 0.1163 - acc: 0.9558   \n",
      "Epoch 81/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1088 - acc: 0.9589   \n",
      "Epoch 82/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1084 - acc: 0.9582   \n",
      "Epoch 83/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1078 - acc: 0.9579   \n",
      "Epoch 84/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1015 - acc: 0.9612   \n",
      "Epoch 85/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1082 - acc: 0.9586   \n",
      "Epoch 86/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1065 - acc: 0.9590   \n",
      "Epoch 87/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1069 - acc: 0.9587   \n",
      "Epoch 88/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1072 - acc: 0.9590   \n",
      "Epoch 89/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1046 - acc: 0.9609   \n",
      "Epoch 90/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1028 - acc: 0.9613   \n",
      "Epoch 91/200\n",
      "62500/62500 [==============================] - 122s - loss: 0.1044 - acc: 0.9597   \n",
      "Epoch 92/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1020 - acc: 0.9608   \n",
      "Epoch 93/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1017 - acc: 0.9608   \n",
      "Epoch 94/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0973 - acc: 0.9636   \n",
      "Epoch 95/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0997 - acc: 0.9617   \n",
      "Epoch 96/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1036 - acc: 0.9602   \n",
      "Epoch 97/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1018 - acc: 0.9612   \n",
      "Epoch 98/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.1007 - acc: 0.9610   \n",
      "Epoch 99/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0986 - acc: 0.9620   \n",
      "Epoch 100/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9608\n",
      "Epoch 00099: reducing learning rate to 0.0001250000059371814.\n",
      "62500/62500 [==============================] - 123s - loss: 0.1014 - acc: 0.9608   \n",
      "Epoch 101/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0971 - acc: 0.9625   \n",
      "Epoch 102/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0973 - acc: 0.9632   \n",
      "Epoch 103/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0943 - acc: 0.9639   \n",
      "Epoch 104/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0947 - acc: 0.9640   \n",
      "Epoch 105/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0979 - acc: 0.9623   \n",
      "Epoch 106/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0988 - acc: 0.9619   \n",
      "Epoch 107/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0978 - acc: 0.9621   \n",
      "Epoch 108/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0924 - acc: 0.9651   \n",
      "Epoch 109/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0908 - acc: 0.9648   \n",
      "Epoch 110/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0955 - acc: 0.9642   \n",
      "Epoch 111/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0915 - acc: 0.9650   \n",
      "Epoch 112/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0942 - acc: 0.9642   \n",
      "Epoch 113/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0948 - acc: 0.9640   \n",
      "Epoch 114/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9647\n",
      "Epoch 00113: reducing learning rate to 6.25000029685907e-05.\n",
      "62500/62500 [==============================] - 123s - loss: 0.0932 - acc: 0.9647   \n",
      "Epoch 115/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0919 - acc: 0.9645   \n",
      "Epoch 116/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0920 - acc: 0.9641   \n",
      "Epoch 117/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0896 - acc: 0.9655   \n",
      "Epoch 118/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0905 - acc: 0.9653   \n",
      "Epoch 119/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0929 - acc: 0.9638   \n",
      "Epoch 120/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0911 - acc: 0.9644   \n",
      "Epoch 121/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0908 - acc: 0.9654   \n",
      "Epoch 122/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0911 - acc: 0.9655   \n",
      "Epoch 123/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9647\n",
      "Epoch 00122: reducing learning rate to 3.125000148429535e-05.\n",
      "62500/62500 [==============================] - 123s - loss: 0.0915 - acc: 0.9647   \n",
      "Epoch 124/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0894 - acc: 0.9657   \n",
      "Epoch 125/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0908 - acc: 0.9648   \n",
      "Epoch 126/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0882 - acc: 0.9655   \n",
      "Epoch 127/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0889 - acc: 0.9660   \n",
      "Epoch 128/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0865 - acc: 0.9670   \n",
      "Epoch 129/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0881 - acc: 0.9660   \n",
      "Epoch 130/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0926 - acc: 0.9639   \n",
      "Epoch 131/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0931 - acc: 0.9641   \n",
      "Epoch 132/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0888 - acc: 0.9656   \n",
      "Epoch 133/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0900 - acc: 0.9656   \n",
      "Epoch 134/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0867 - acc: 0.9672   \n",
      "Epoch 135/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0880 - acc: 0.9659   \n",
      "Epoch 136/200\n",
      "62500/62500 [==============================] - 124s - loss: 0.0905 - acc: 0.9649   \n",
      "Epoch 137/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0895 - acc: 0.9657   \n",
      "Epoch 138/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0870 - acc: 0.9662   \n",
      "Epoch 139/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0880 - acc: 0.9670   \n",
      "Epoch 140/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9656\n",
      "Epoch 00139: reducing learning rate to 1.5625000742147677e-05.\n",
      "62500/62500 [==============================] - 123s - loss: 0.0887 - acc: 0.9657   \n",
      "Epoch 141/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0877 - acc: 0.9659   \n",
      "Epoch 142/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0894 - acc: 0.9652   \n",
      "Epoch 143/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0876 - acc: 0.9671   \n",
      "Epoch 144/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0877 - acc: 0.9664   \n",
      "Epoch 145/200\n",
      "62375/62500 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9661\n",
      "Epoch 00144: reducing learning rate to 7.812500371073838e-06.\n",
      "62500/62500 [==============================] - 123s - loss: 0.0902 - acc: 0.9661   \n",
      "Epoch 146/200\n",
      "62500/62500 [==============================] - 123s - loss: 0.0867 - acc: 0.9668   \n",
      "Epoch 00145: early stopping\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # Ensure reproducibility\n",
    "\n",
    "model.model.summary()\n",
    "model.train(gt_imgs, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to disk\n",
    "model.save('saved_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
