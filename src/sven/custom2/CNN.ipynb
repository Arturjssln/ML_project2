{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Construct a CNN classifier. \"\"\"\n",
    "        \n",
    "        self.patch_size = 16\n",
    "        self.window_size = 72\n",
    "        self.padding = (self.window_size - self.patch_size) // 2\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Initialize or reset this model. \"\"\"\n",
    "        patch_size = self.patch_size\n",
    "        window_size = self.window_size\n",
    "        padding = self.padding\n",
    "        nb_classes = 2\n",
    "        \n",
    "        # Size of pooling area for max pooling\n",
    "        pool_size = (2, 2)\n",
    "        input_shape = (window_size, window_size, 3)\n",
    "\n",
    "        reg = 1e-6 # L2 regularization factor (used on weights, but not biases)\n",
    "\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Convolution2D(64, 5, 5, # 64 5x5 filters\n",
    "                                border_mode='same',\n",
    "                                input_shape=input_shape\n",
    "                               ))\n",
    "        self.model.add(LeakyReLU(alpha=0.1))\n",
    "        self.model.add(MaxPooling2D(pool_size=pool_size, border_mode='same'))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Convolution2D(128, 3, 3, # 128 3x3 filters\n",
    "                                border_mode='same'\n",
    "                               ))\n",
    "        self.model.add(LeakyReLU(alpha=0.1))\n",
    "        self.model.add(MaxPooling2D(pool_size=pool_size, border_mode='same'))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Convolution2D(256, 3, 3, # 256 3x3 filters\n",
    "                                border_mode='same'\n",
    "                               ))\n",
    "        self.model.add(LeakyReLU(alpha=0.1))\n",
    "        self.model.add(MaxPooling2D(pool_size=pool_size, border_mode='same'))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Convolution2D(256, 3, 3, # 256 3x3 filters\n",
    "                                border_mode='same'\n",
    "                               ))\n",
    "        self.model.add(LeakyReLU(alpha=0.1))\n",
    "        self.model.add(MaxPooling2D(pool_size=pool_size, border_mode='same'))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, W_regularizer=l2(reg)\n",
    "                            )) # Fully connected layer (128 neurons)\n",
    "        self.model.add(LeakyReLU(alpha=0.1))\n",
    "        self.model.add(Dropout(0.5))\n",
    "\n",
    "        self.model.add(Dense(nb_classes, W_regularizer=l2(reg)\n",
    "                            ))\n",
    "        #self.model.add(Activation('softmax')) # Not needed since we use logits\n",
    "        \n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train this model with the given dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        patch_size = self.patch_size\n",
    "        window_size = self.window_size\n",
    "        padding = self.padding\n",
    "        \n",
    "        print('Training set shape: ', X.shape)\n",
    "        samples_per_epoch = X.shape[0]*X.shape[1]*X.shape[2]//1024 # Arbitrary value\n",
    "        \n",
    "        # Pad training set images (by appling mirror boundary conditions)\n",
    "        X_new = np.empty((X.shape[0],\n",
    "                         X.shape[1] + 2*padding, X.shape[2] + 2*padding,\n",
    "                         X.shape[3]))\n",
    "        Y_new = np.empty((Y.shape[0],\n",
    "                         Y.shape[1] + 2*padding, Y.shape[2] + 2*padding))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_new[i] = pad_image(X[i], padding)\n",
    "            Y_new[i] = pad_image(Y[i], padding)\n",
    "        X = X_new\n",
    "        Y = Y_new\n",
    "            \n",
    "        batch_size = 32#125\n",
    "        nb_classes = 2\n",
    "        nb_epoch = 20\n",
    "\n",
    "        def softmax_categorical_crossentropy(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Uses categorical cross-entropy from logits in order to improve numerical stability.\n",
    "            This is especially useful for TensorFlow (less useful for Theano).\n",
    "            \"\"\"\n",
    "            return losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "        opt = Adam(lr=0.001) # Adam optimizer with default initial learning rate\n",
    "        self.model.compile(loss=softmax_categorical_crossentropy,\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        np.random.seed(3) # Ensure determinism\n",
    "        \n",
    "        def generate_minibatch():\n",
    "            \"\"\"\n",
    "            Procedure for real-time minibatch creation and image augmentation.\n",
    "            This runs in a parallel thread while the model is being trained.\n",
    "            \"\"\"\n",
    "            while 1:\n",
    "                # Generate one minibatch\n",
    "                X_batch = np.empty((batch_size, window_size, window_size, 3))\n",
    "                Y_batch = np.empty((batch_size, 2))\n",
    "                for i in range(batch_size):\n",
    "                    # Select a random image\n",
    "                    idx = np.random.choice(X.shape[0])\n",
    "                    shape = X[idx].shape\n",
    "                    \n",
    "                    # Sample a random window from the image\n",
    "                    center = np.random.randint(window_size//2, shape[0] - window_size//2, 2)\n",
    "                    sub_image = X[idx][center[0]-window_size//2:center[0]+window_size//2,\n",
    "                                       center[1]-window_size//2:center[1]+window_size//2]\n",
    "                    gt_sub_image = Y[idx][center[0]-patch_size//2:center[0]+patch_size//2,\n",
    "                                          center[1]-patch_size//2:center[1]+patch_size//2]\n",
    "                    \n",
    "                    # The label does not depend on the image rotation/flip (provided that the rotation is in steps of 90°)\n",
    "                    threshold = 0.25\n",
    "                    label = (np.array([np.mean(gt_sub_image)]) > threshold) * 1\n",
    "                    \n",
    "                    # Image augmentation\n",
    "                    # Random flip\n",
    "                    if np.random.choice(2) == 0:\n",
    "                        # Flip vertically\n",
    "                        sub_image = np.flipud(sub_image)\n",
    "                    if np.random.choice(2) == 0:\n",
    "                        # Flip horizontally\n",
    "                        sub_image = np.fliplr(sub_image)\n",
    "                    \n",
    "                    # Random rotation in steps of 90°\n",
    "                    num_rot = np.random.choice(4)\n",
    "                    sub_image = np.rot90(sub_image, num_rot)\n",
    "\n",
    "                    label = np_utils.to_categorical(label, nb_classes)\n",
    "                    X_batch[i] = sub_image\n",
    "                    Y_batch[i] = label\n",
    "                    \n",
    "                yield (X_batch, Y_batch)\n",
    "\n",
    "        # This callback reduces the learning rate when the training accuracy does not improve any more\n",
    "        lr_callback = ReduceLROnPlateau(monitor='acc', factor=0.5, patience=5,\n",
    "                                        verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "        \n",
    "        # Stops the training process upon convergence\n",
    "        stop_callback = EarlyStopping(monitor='acc', min_delta=0.0001, patience=11, verbose=1, mode='auto')\n",
    "        \n",
    "        mode_autosave = ModelCheckpoint(\"./weights/best.h5\",monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1, period =1)\n",
    "        \n",
    "        try:\n",
    "            self.model.fit_generator(generate_minibatch(),\n",
    "                            samples_per_epoch=samples_per_epoch,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            verbose=1,\n",
    "                            callbacks=[lr_callback, stop_callback, mode_autosave])\n",
    "        except KeyboardInterrupt:\n",
    "            # Do not throw away the model in case the user stops the training process\n",
    "            pass\n",
    "\n",
    "        print('Training completed')\n",
    "        \n",
    "    def save(self, filename):\n",
    "        \"\"\" Save the weights of this model. \"\"\"\n",
    "        self.model.save_weights(filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        \"\"\" Load the weights for this model from a file. \"\"\"\n",
    "        self.model.load_weights(filename)\n",
    "        \n",
    "    def classify(self, X):\n",
    "        \"\"\"\n",
    "        Classify an unseen set of samples.\n",
    "        This method must be called after \"train\".\n",
    "        Returns a list of predictions.\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks\n",
    "        img_patches = create_patches(X, self.patch_size, 16, self.padding)\n",
    "        \n",
    "        # Run prediction\n",
    "        Z = self.model.predict(img_patches)\n",
    "        Z = (Z[:,0] < Z[:,1]) * 1\n",
    "        \n",
    "        # Regroup patches into images\n",
    "        return group_patches(Z, X.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
